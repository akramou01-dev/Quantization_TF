{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f425b4",
   "metadata": {},
   "source": [
    "Quantization is a technique to optimize the model's size to being able to be deployed in small devices which contains micro-controlers with very limited resources.\n",
    "\n",
    "In the deep model, the weights are very important for the model and they generaly are of type float, for saving floats (with best precision)  in the Memory we need 8 bytes, but if we can set an approximation of these floats and convert them into integers for exemple we can reduce the storage in the memory to 1 byte for each number.\n",
    "\n",
    "we change the types of the weight's values in order to reduce the size of each value \n",
    "\n",
    "The quantization is not converting all the weight's type blindly, but there is an algorithm for converting the values type \n",
    "\n",
    "2 type of Quantization in tf : \n",
    "    \n",
    "1. Post training quantization : we take the tf trained model and converting it with tf.lite to smaller model , if we add the quantization we will get a more smaller one, this approach has less work but it loss accuracy\n",
    "2. Quantization aware training : we take our model we apply the quantization on it and we train it again , after that we apply the tf.lite to convert it to smaller model, in this approach we have little more work but we have also good accuracy \n",
    "\n",
    "\n",
    "un trés bon shéma explicatif peut etre trouver ici https://www.tensorflow.org/lite/performance/post_training_quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb6542",
   "metadata": {},
   "source": [
    "## Post Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6eaa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5a654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tf.lite without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./Models/1\")\n",
    "tfLite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "380d43c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319532"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfLite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3050dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"./Models/1\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT ]\n",
    "tfLite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abaad179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84496"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfLite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b475a",
   "metadata": {},
   "source": [
    "we see that the model with quantization is more smaller then the other where we use juste the conversion to the TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ff82b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save the models into files \n",
    "with open(\"tflite_model.tflite\",\"wb\") as f : \n",
    "    f.write(tfLite_model)\n",
    "\n",
    "with open('tflite_quant_model.tflite',\"wb\") as f : \n",
    "    f.write(tfLite_quant_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "97ac4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model \n",
    "from tensorflow import keras \n",
    "model = keras.models.load_model(\"./Models/1\")\n",
    "model.compile(optimizer= 'adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(),metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2f7acc",
   "metadata": {},
   "source": [
    "## Quantization aware training \n",
    "\n",
    "we are going to use the quantize_model for quantized the model, this fonction will preserve the weights of the original model, and create a new model with the same weights that is adapted to the quantization but it is note quantized. Noting that the quantize_model fonction eliminate the optimizer from the original model so we need to re-compile our new model. After this step we will train the resulting model  so that we have good accuracy .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "595d54ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad9338c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow_model_optimization \n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1312db5",
   "metadata": {},
   "source": [
    "So from our original model (saved model) we are going to create a new model that is adapted to the quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "19cbb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the fonction that we can reuse it \n",
    "quantize_model = tfmot.quantization.keras.quantize_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f30db59a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# q_aware for quantized aware model \n",
    "# the q_aware_model is a new model with the same weights, without optimizer , \n",
    "q_aware_model = quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "665f7777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_6 (QuantizeLa (None, 784)               3         \n",
      "_________________________________________________________________\n",
      "quant_dense_5 (QuantizeWrapp (None, 100)               78505     \n",
      "_________________________________________________________________\n",
      "quant_dense_6 (QuantizeWrapp (None, 10)                1015      \n",
      "=================================================================\n",
      "Total params: 79,523\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 13\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# we need to compile the new aware model \n",
    "q_aware_model.compile(optimizer = \"adam\", loss =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=[\"accuracy\"] )\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37642539",
   "metadata": {},
   "source": [
    "Now we have a new model that is adapted to the quantization but it's not trained, so we are going to train on our training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c010f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(len(train_images),28*28)\n",
    "test_images = test_images.reshape(len(test_images),28*28)\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# creating a subset of 10000 samples instead of 60000\n",
    "\n",
    "train_images = train_images[:10000]\n",
    "train_labels = train_labels[:10000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "790b6f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "11525b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "265/282 [===========================>..] - ETA: 0s - loss: 1.5215 - accuracy: 0.9812WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "282/282 [==============================] - 1s 3ms/step - loss: 1.5204 - accuracy: 0.9816 - val_loss: 1.4975 - val_accuracy: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25696b76490>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(train_images,train_labels,epochs=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "88c5e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,base_line_model_accuracy = model.evaluate(test_images,test_labels,verbose=0)\n",
    "_,q_aware_accuracy = q_aware_model.evaluate(test_images,test_labels,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a409084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base model test accuracy : 0.9750000238418579\n",
      "The quantification aware model accuracy: 0.97079998254776\n"
     ]
    }
   ],
   "source": [
    "print(f\"The base model test accuracy : {base_line_model_accuracy}\")\n",
    "print(f\"The quantification aware model accuracy: {q_aware_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2325c",
   "metadata": {},
   "source": [
    "now we have a quantized model with weights of type int8 and activation of uint8\n",
    "\n",
    "using the tfLite to convert the model and optimze the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a1103e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\21355\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\21355\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\21355\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\21355\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <tensorflow_model_optimization.python.core.quantization.keras.quantizers.AllValuesQuantizer object at 0x000002569B41E880> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\21355\\AppData\\Local\\Temp\\tmptdzfcvmb\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\21355\\AppData\\Local\\Temp\\tmptdzfcvmb\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857c10f",
   "metadata": {},
   "source": [
    "we are now going to evaluate the quantized model on the test set and for this we will create an evaluate fonction \n",
    "\n",
    "for evaluating a tfLite model we need to use the tf.lite.Interpreter fonction that helps us to get new predicions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "90700ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(interpreter):\n",
    "    # first we are going to extract informations about the input and output tensors of the interpreter \n",
    "    # for that we are going to use the .get_input/output_details() fonction which will give us a list of dicts that contains some informations about each tensor\n",
    "    \n",
    "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "    prediction_digits = []\n",
    "    for i, test_image in enumerate(test_images): \n",
    "        if i %1000==0: \n",
    "            print(f\"Evaluated on {i} results \")\n",
    "        # pre-processing: add batch dimension (we need to add 1 dimension to each image because we train the model on batches of data) \n",
    "        #                    using the np.expand_dims that add 1 dimension to our data\n",
    "        #                 convert data to float32 that match with the model's input .astype(np.float32)\n",
    "        # note that there is no remarkable diffrence between the float 64 and the float 32\n",
    "        test_image = np.expand_dims(test_image,axis=0).astype(np.float32)\n",
    "        # now we add the value of the test_image to the tensor \n",
    "        interpreter.set_tensor(input_index , test_image)\n",
    "        # Run inference.\n",
    "        # we need to do .allocate_tensors() \n",
    "        interpreter.invoke()\n",
    "        \n",
    "#         print(interpreter.tensor(input_index))\n",
    "#         print(interpreter.tensor(output_index))\n",
    "        # now we need to do post-processing : remove the batch dimension and extract the class that has highest porba\n",
    "        \n",
    "        output = interpreter.tensor(output_index)\n",
    "#         print(output)\n",
    "        \n",
    "        digit = np.argmax(output()[0])\n",
    "        prediction_digits.append(digit)\n",
    "#         if digit != test_labels[i]:\n",
    "#             print(digit,test_labels[i])\n",
    "\n",
    "        \n",
    "    prediction_digits = np.array(prediction_digits)\n",
    "    accuracy = (prediction_digits == test_labels).mean()\n",
    "    return accuracy \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "fda0fe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results \n",
      "Evaluated on 1000 results \n",
      "Evaluated on 2000 results \n",
      "Evaluated on 3000 results \n",
      "Evaluated on 4000 results \n",
      "Evaluated on 5000 results \n",
      "Evaluated on 6000 results \n",
      "Evaluated on 7000 results \n",
      "Evaluated on 8000 results \n",
      "Evaluated on 9000 results \n",
      "Quantized TFLite model  accuracy : 0.9709\n",
      "Quantized TF model  accuracy : 0.97079998254776\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content = quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_lite_acc = evaluate(interpreter)\n",
    "print(f\"Quantized TFLite model  accuracy : {test_lite_acc}\")\n",
    "print(f\"Quantized TF model  accuracy : {q_aware_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ef23d",
   "metadata": {},
   "source": [
    "After the evaluating of the TFLite model, we note that the TFLite and the TF model has the same accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ed3873c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.3045654296875\n",
      "Quantized model in Mb: 0.07837677001953125\n"
     ]
    }
   ],
   "source": [
    "with open('quantized_model', 'wb') as f:\n",
    "  f.write(quantized_tflite_model)\n",
    "\n",
    "with open(\"float_model\", 'wb') as f:\n",
    "  f.write(float_tflite_model)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(\"./float_model\") / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(\"./quantized_model\") / float(2**20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6fe50c",
   "metadata": {},
   "source": [
    "we can say that the quantized model is almost x100 times more smaller then the float model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c38b22",
   "metadata": {},
   "source": [
    "so as conclusion, we can say that the aware training quantization does not loss any accuracy and also when we optimize our model by converting it to TFLite model we dont loss the accuracy on the new model. So for the EDGE Devices, the TF quantization is very useful and performence when we have limited resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1b68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3a66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
